---
title: "Detect Credit Card Fraud with Machine Learning in R"
author: "Ebenezer Bediam Nonbo"
date: "2023-04-08"
output: html_document
---




```{r}
## Importing the Datasets

#We are importing the datasets that contain transactions made by credit cards-
 
library(readxl)
creditcard=read_excel("D:/BEDIAM/Ebenezer/Logiciels/R/Machine_learning/creditcard.xlsx")
str(creditcard)
names(creditcard)

```


##

```{r}
# Loading required package 

library(ranger)
library(caret)
library(data.table)

```


## 

```{r}
## Data Exploration

# In this section of the fraud detection ML project, we will explore the data that is contained in the creditcard_data dataframe
 
 dim(creditcard)
 table(creditcard$Class)
 summary(creditcard$Amount)
 names(creditcard)
 var(creditcard$Amount)
 sd(creditcard$Amount)


```


## 

```{r}
## Data Manipulation

#In this section of the R data science project, we will scale our data using the scale() function. We will apply this to the amount component of our creditcard_data amount. Scaling is also known as feature standardization. With the help of scaling, the data is structured according to a specified range

creditcard$V4=as.numeric(creditcard$V4)
creditcard$V3=as.numeric(creditcard$V3)
creditcard$V2=as.numeric(creditcard$V2)
creditcard$V1=as.numeric(creditcard$V1)
creditcard$Amount=as.numeric(creditcard$Amount)
creditcard$Amount=scale(creditcard$Amount)
NewData=creditcard[,-c(1)]
head(NewData)


```


##

```{r}
## Data Modeling

#After we have standardized our entire dataset, we will split our dataset into training set as well as test set with a split ratio of 0.80. This means that 80% of our data will be attributed to the train_data whereas 20% will be attributed to the test data

library(caTools)
set.seed(123)
data_sample = sample.split(NewData$Class,SplitRatio=0.80)
train_data = subset(NewData,data_sample==TRUE)
test_data = subset(NewData,data_sample==FALSE)
dim(train_data)
dim(test_data)

```

## 

```{r}
## Fitting Logistic Regression Model

#In this section of credit card fraud detection project, we will fit our first model. We will begin with logistic regression. A logistic regression is used for modeling the outcome probability of a class such as pass/fail, positive/negative and in our case â€“ fraud/not fraud

Logistic_Model=glm(Class~.,test_data,family=binomial())
summary(Logistic_Model)
plot(Logistic_Model)

```


## 

```{r}
#In order to assess the performance of our model, we will delineate the ROC curve. ROC is also known as Receiver Optimistic Characteristics. For this, we will first import the ROC package and then plot our ROC curve to analyze its performance

library(pROC)
lr.predict <- predict(Logistic_Model,train_data, probability = TRUE)
auc.gbm = roc(test_data$Class, lr.predict, plot = TRUE, col = "blue")


```

##

```{r}
## Fitting a Decision Tree Model

#In this section, we will implement a decision tree algorithm. Decision Trees to plot the outcomes of a decision. These outcomes are basically a consequence through which we can conclude as to what class the object belongs to. We will now implement our decision tree model and will plot it using the rpart.plot() function

library(rpart)
library(rpart.plot)
decisionTree_model <- rpart(Class ~ . , creditcard, method = 'class')
predicted_val <- predict(decisionTree_model, creditcard, type = 'class')
probability <- predict(decisionTree_model, creditcard, type = 'prob')
rpart.plot(decisionTree_model)

```

## 

```{r}
##Artificial Neural Network

# The ANN models are able to learn the patterns using the historical data and are able to perform classification on the input data. We import the neuralnet package that would allow us to implement our ANNs. Then we proceeded to plot it using the plot() function. Now, in the case of Artificial Neural Networks, there is a range of values that is between 1 and 0. We set a threshold as 0.5, that is, values above 0.5 will correspond to 1 and the rest will be 0

library(neuralnet)
ANN_model =neuralnet (Class~.,train_data,linear.output=FALSE)
plot(ANN_model)

predANN=compute(ANN_model,test_data)
resultANN=predANN$net.result
resultANN=ifelse(resultANN>0.5,1,0)

```

## 

```{r}
## Gradient Boosting (GBM)

#Gradient Boosting is a popular machine learning algorithm that is used to perform classification and regression tasks. This model comprises of several underlying ensemble models like weak decision trees. These decision trees combine together to form a strong model of gradient boosting. 

library(gbm, quietly=TRUE)

# Get the time to train the GBM model
system.time(
       model_gbm <- gbm(Class ~ .
               , distribution = "bernoulli"
               , data = rbind(train_data, test_data)
               , n.trees = 500
               , interaction.depth = 3
               , n.minobsinnode = 100
               , shrinkage = 0.01
               , bag.fraction = 0.5
               , train.fraction = nrow(train_data) / (nrow(train_data) + nrow(test_data))
)
)
# Determine best iteration based on test data
gbm.iter = gbm.perf(model_gbm, method = "test")


```

##

```{r}
model.influence = relative.influence(model_gbm, n.trees = gbm.iter, sort. = TRUE)
#Plot the gbm model

plot(model_gbm)

```

## 

```{r}
# Plot and calculate AUC on test data
gbm_test = predict(model_gbm, newdata = test_data, n.trees = gbm.iter)
gbm_auc = roc(test_data$Class, gbm_test, plot = TRUE, col = "red")

```

## 

```{r}
print(gbm_auc)

```

##

```{r}


```

## 
